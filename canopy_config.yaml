# ===================================================================
#  Custom configuration file for Canopy Server with Cohere & OpenAI.
# ===================================================================

# ---------------------------------------------------------------------------------
system_prompt: &system_prompt |
  Use the documents to answer the user question at the next messages. The documents are retrieved from a knowledge
  database and you should use only the facts from the documents to answer. Always remember to include the source to
  the documents you used from their 'source' field in the format 'Source: $SOURCE_HERE'.
  If you don't know the answer, just say that you don't know, don't try to make up an answer, use the documents.
  Don't address the documents directly, but use them to answer the user question like it's your own knowledge.
query_builder_prompt: &query_builder_prompt |
  Your task is to formulate search queries for a search engine, to assist in responding to the user's question.
  You should break down complex questions into sub-queries if needed.


# -------------------------------------------------------------------------------------------
# Tokenizer configuration
# -------------------------------------------------------------------------------------------
tokenizer:
  type: CohereHFTokenizer
  params:
    model_name: Cohere/Command-nightly


chat_engine:
  # -------------------------------------------------------------------------------------------------------------
  # Chat engine configuration
  # The chat engine is the main component of the server, generating responses to the `/chat.completion` API call.
  # The configuration is recursive, so that each component contains a subsection for each of its sub components.
  # -------------------------------------------------------------------------------------------------------------
  params:
    max_prompt_tokens: 4096             # The maximum number of tokens to use for input prompt to the LLM
    max_generated_tokens: null          # Leaving `null` will use the default of the underlying LLM
    max_context_tokens: null            # Leaving `null` will use 70% of `max_prompt_tokens`
    system_prompt: *system_prompt       # The chat engine's system prompt for calling the LLM
    allow_model_params_override: false  # Whether to allow overriding the LLM's parameters in an API call

  history_pruner:             # How to prune messages if chat history is too long. Options: [RecentHistoryPruner, RaisingHistoryPruner]
    type: RecentHistoryPruner
    params:
      min_history_messages: 1             # Minimal number of messages to keep in history
  # -------------------------------------------------------------------------------------------------------------
  # LLM configuration
  # -------------------------------------------------------------------------------------------------------------
  llm: &llm
    # -------------------------------------------------------------------------------------------------------------
    # LLM configuration
    # Configuration of the LLM (Large Language Model)
    # -------------------------------------------------------------------------------------------------------------
    type: OpenAILLM                     # Options: [OpenAILLM, AzureOpenAILLM]
    params:
      model_name: gpt-3.5-turbo         # The name of the model to use.
      # You can add any additional parameters which are supported by the LLM's `ChatCompletion()` API. The values
      # set here will be used in every LLM API call, but may be overridden if `allow_model_params_override` is true.
      # temperature: 0.7
      # top_p: 0.9

  # --------------------------------------------------------------------
  # Configuration for the QueryBuilder subcomponent of the chat engine.
  # --------------------------------------------------------------------
  query_builder:
    # -------------------------------------------------------------------------------------------------------------
    # LLM configuration
    # Configuration of the LLM (Large Language Model)
    # -------------------------------------------------------------------------------------------------------------
    type: FunctionCallingQueryGenerator # Options: [FunctionCallingQueryGenerator, LastMessageQueryGenerator, InstructionQueryGenerator]
    params:
      prompt: *query_builder_prompt     # The query builder's system prompt for calling the LLM
      function_description:             # A function description passed to the LLM's `function_calling` API
        Query search engine for relevant information

    llm:  # The LLM that the query builder will use to generate queries. Leave `*llm` to use the chat engine's LLM
      <<: *llm

  # -------------------------------------------------------------------------------------------------------------
  # ContextEngine configuration
  # -------------------------------------------------------------------------------------------------------------
  context_engine:
    # -----------------------------------------------------------------------------------------------------------
    # KnowledgeBase configuration
    # -----------------------------------------------------------------------------------------------------------
    knowledge_base:
      params:
        default_top_k: 100

      # --------------------------------------------------------------------------
      # Configuration for the RecordEncoder subcomponent of the knowledge base.
      # --------------------------------------------------------------------------
      record_encoder:
        type: CohereRecordEncoder
        params:
          model_name:                   # The name of the model to use for encoding
            "embed-english-v3.0"
          batch_size: 100               # The number of document chunks to encode in each call to the encoding model

      reranker:
        type: CohereReranker
        params:
          top_n: 5


create_index_params:
  # -------------------------------------------------------------------------------------------
  # Initialization parameters to be passed to create a canopy index. These parameters will
  # be used when running "canopy new".
  # -------------------------------------------------------------------------------------------
  metric: cosine
  spec:
    serverless:
      cloud: aws
      region: us-east-1
    # For pod indexes you can pass the spec with the key "pod" instead of "serverless"
    # See the example below:
#    pod:
#      environment: eu-west1-gcp
#      pod_type: p1.x1
#      # Here you can specify here replicas, shards, pods, metadata_config if needed.